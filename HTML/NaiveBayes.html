<HTML>
<head>
<body style="background-color:#F6F5F5;">

<style>

body {
	font-family: verdana;
    background-color : #F6F5F5 ;
    position : relative
    style: #232435;

}

.topnav {
  overflow: hidden;
  background-color: #1F1A40;
}

.topnav a {
  float: left;
  color: #BBD8F2;
  text-align: center;
  padding: 12px 14px;
  text-decoration: none;
  font-size: 17px;
}

.topnav a:hover {
  background-color: #D3E0EA;
  color: black;
}

.topnav a.active {
  background-color: #FFFFFF;
  color: white;
}

.headerLogo{
  top:200px;
  height:250px;
  width:200px;
  line-height:200px;

  overflow:hidden;
 
  <!-- top:200px; height:900px; width:1450px; line-height:500px -->
}
.center {
  display: block;
  margin-left: auto;
  margin-right: auto;
  width: 100%;
}

h1 {text-align: left;}
p {text-align: left;}
div {text-align: left;}


ul {
    list-style-type: none;
    margin: 0;
    padding: 0;
    overflow: hidden;
    background-color: #34495E;
    max-width:1500px;
}

li {
    float: left;
}

li a, .dropbtn {
    display: inline-block;
    color: white;
    text-align: center;
    padding: 14px 16px;
    text-decoration: none;
}

li a:hover, .dropdown:hover .dropbtn {
    background-color: grey;
}

li.dropdown {
    display: inline-block;
}

.dropdown-content {
    display: none;
    position: absolute;
    background-color: white;
    min-width: 160px;
    
    
}

.dropdown-content a {
    color: black;
    padding: 12px 16px;
    text-decoration: none;
    display: block;
    text-align: left;
    
}



.dropdown:hover .dropdown-content {
    display: block;
    opacity: 1;
}
td {
    padding: 5px;
    text-align: left;
    width: 500px;
}

tr{
   padding: 0px;
   text-align: top;
   background-color:#ffffff
}

img:hover {
  opacity: 1;
}

</style>


<ul>
    
   <li><a href="https://sonalipednekar.georgetown.domains/#home" >  About Me </a></li>
    
    <li><a href="https://sonalipednekar.georgetown.domains/ANLY501/Introduction.html" >  Introduction </a></li>
    
    <li><a href="https://sonalipednekar.georgetown.domains/ANLY501/DataGathering.html" >  Data Gathering </a></li>
    
    
    <li><a href="https://sonalipednekar.georgetown.domains/ANLY501/DataCleaning.html" >  Data Cleaning </a></li>
    
    <li><a href="https://sonalipednekar.georgetown.domains/ANLY501/ExploringData.html" >  Exploring Data </a></li>
    
    <li class="dropdown">
        <a href="javascript:void(0)" class="dropbtn">Clustering</a>
    
        <div class="dropdown-content">
        <a href="https://sonalipednekar.georgetown.domains/ANLY501/Clustering_with_R.html" >Clustering with R</a>
        <a href="https://sonalipednekar.georgetown.domains/ANLY501/Cluster_with_Python.html" >Clustering with Python</a>
        </div>
    </li>

    <li><a href="https://sonalipednekar.georgetown.domains/ANLY501/ARMandNetworking.html" >  ARM and Networking </a></li>
    
    <li class="dropdown">
        <a href="javascript:void(0)" class="dropbtn">Decision Trees</a>
    
        <div class="dropdown-content">
        <a href="https://sonalipednekar.georgetown.domains/ANLY501/DecisionTrees_with_Python.html" >Decision Trees with R</a>
        <a href="https://sonalipednekar.georgetown.domains/ANLY501/DecisionTrees_with_Python.html" >Decision Trees with Python</a>
        </div>
    </li>
    
    
    <li><a href="#Naive Bayes" >  Naive Bayes </a></li>
    
    <li><a href="https://sonalipednekar.georgetown.domains/ANLY501/SVM.html" >  SVM </a></li>
   
    <li><a href="https://sonalipednekar.georgetown.domains/ANLY501/Conclusions.html" >  Conclusions </a></li>
    
    
    
</div>
</ul>

<div class="w3-container w3-border city" id="Naive Bayes" >
<h1><center><b>Naive Bayes</b></center></h1>
<p><center>This page contains the Naive Bayes code and visualizations done in R for Record data and in Python for Text data. 

<h3><b>What is Naive Bayes</b></h3>
<p> It is a classification technique based on Bayes Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature. Naive Bayes model is easy to build and particularly useful for very large data sets. Along with simplicity, Naive Bayes is known to outperform even highly sophisticated classification methods. </p>
<br>			
<h2><b>Naive Bayes in R</b></h2>
<p> The naive bayes is created using record dataset in R. It is the same record dataset used for decision trees. The dataset has been downloaded from UCI Machine learning repository. The link for the source can be found <a href="https://archive.ics.uci.edu/ml/datasets/Census+Income" target="_blank">here</a> This dataset consists of various features like age, workclass, education, marital status, relationship, race, sex, hours per week. The label column contains the data of whether the person is rich (salary greater than or equal to 50k) or poor(salary less than $50k).  </p>

<p><center>The snapshot of the dataset and the link to the csv file is attached below.</center></p>
<center><img alt="dt_r_data_small.jpg" height="400px" src="dt_r_data_small.jpg" width="600px" /></center>
			<center><a href="dt_r_data.jpg" target="new">View </a></center>
			<a href="dt_r_data.jpg" target="new"> </a>

			<center><a href="https://sonalipednekar.georgetown.domains/ANLY501/income_evaluation.csv">Download csv file</a></center>
<br>			
<h3><b>Cleaning and formatting the dataset to the required format</b></h3>
<p> The code to create clean and format the datasets can be found  <a href="https://sonalipednekar.georgetown.domains/ANLY501/NB_R.R" target="_blank">here</a> </p>
<p>There are a lot of unwated columns in the dataframe. These columns are dropped from the dataframe, retaining only the necessary columns. The dataset is checked for NA values, and all the NA values are removed. Checking the balance of the dataset and label is very important before performing decision trees, as unbalanced dataset may lead to over or underfitting. </p>

<p><center>The snapshot of the label column (income) before and after balancing. </center></p>
<img alt="income_before_bal.jpeg" height="400px" src="income_before_bal.jpeg" width="600px" />


<img alt="balance_label.jpeg" height="400px" src="balance_label.jpeg" width="600px" />

<br>
<h3><b>Model Building</b></h3>
<p> The code to build the model can be found <a href="https://sonalipednekar.georgetown.domains/ANLY501/NB_R.R" target="_blank">here</a> </p>
<p>Before building the model, the dataset is split into training and testing sets. The split ratio is 0.75 of the total data in the training set and 0.25 data in the testing set. The naive bayes model is trained using the training dataset and then the model is tested using labels from the testing dataset </p>

<p><center>The snapshot of the naive bayes are attached below.</center></p>
<img alt="Conf_NB.jpeg" height="400px" src="Conf_NB.jpeg" width="600px" />
<img alt="conf_matrix_nb.jpg" height="400px" src="conf_matrix_nb.jpg" width="600px" />
<p><center>The graph to show the key features from the model is attached below.</center></p>
<img alt="Cross_val_nb.jpeg" height="400px" src="Cross_val_nb.jpeg" width="600px" />

<h3><b>Conclusion</b></h3>
<p>The intention was to perform naive bayes to help predict the income class of a person given the different features. The accuracy of the model (79%) is pretty decent in predicting the income group. According to the above graph, the key features to predict the income group are age, hours worked per week and sex. It makes sense that the variable age and hours worked per week have a significant impact on income.  </p>

<br>
<h2><b>Naive Bayes in Python</b></h2>
<p> The naive bayes is created using text dataset in Python.The tweets are extracted on the hashtag "social worker" and hashtag "covid" . The motive behind collecting this text data was to understand the opinion of people regarding social workers and different tweets regarding covid. To get a quick overview of the data, a wordcloud of both the hashtags has been made.</p>
<img alt="wc1_nb.jpg" height="400px" src="wc1_nb.jpg" width="600px" />
<img alt="wc2_nb.jpg" height="400px" src="wc2_nb.jpg" width="600px" />

<h3><b>Cleaning and formatting the dataset to the required format</b></h3>
<p>There are a lot of unwated columns in the dataframe. These columns are dropped from the dataframe, retaining only the necessary columns. The stopwords are removed and the text is tokenized, lemmatized and stemmed. Countvectorizer is applied on the data to convert it to numerical format. Checking the balance of the label is very important before performing decision trees, as unbalanced dataset may lead to over or underfitting.  </p>
<p><center>The snapshot of the dataset and the link to the csv file is attached below.</center></p>
<center><img alt="dt_twitter_small.jpg" height="400px" src="dt_twitter_small.jpg" width="600px" /></center>
			<center><a href="dt_twitter.jpg" target="new">View </a></center>
			<a href="dt_twitter.jpg" target="new"> </a>

			<center><a href="https://sonalipednekar.georgetown.domains/ANLY501/twitter_file_dt.csv">Download csv file</a></center>
<center><img alt="dtm_cv_small.jpg" height="400px" src="dtm_cv_small.jpg" width="600px" /></center>
			<center><a href="dtm_cv.jpg" target="new">View </a></center>
			<a href="dtm_cv.jpg" target="new"> </a>

			<center><a href="https://sonalipednekar.georgetown.domains/ANLY501/DTM_CV_DT.csv">Download csv file</a></center>

<h3><b>Model Building</b></h3>
<p> The code to build the model can be found <a href="https://sonalipednekar.georgetown.domains/ANLY501/NaiveBayes_Python.py" target="_blank">here</a> </p>
<p>Before building the model, the dataset is split into training and testing sets. The split ratio is 0.75 of the total data in the training set and 0.25 data in the testing set. Three different naive bayes models are created. The Naive Bayes models differ due to hypertuning of different parameters. Mainly the alpha values.</p>

<p><b> Naive Bayes Model 1</b></p>
<p>This is the first naive bayes model. In this model, the hyperparameter are alpha = 1. In this model, the accuracy is 95%.</p>
<p><center>The snapshot of the accuracy/heatmap is attached below. </center></p>
<center><img alt="nb1_python.jpg" height="400px" src="nb1_python.jpg" width="600px" /></center>

<p><b> Naive Bayes Model 2</b></p>
<p>This is the second naive bayes model. In this model, the hyperparameter are alpha = 5. In this model, the accuracy is 95%.</p>
<p><center>The snapshot of the accuracy/heatmap is attached below. </center></p>
<center><img alt="nb2_python.jpg" height="400px" src="nb2_python.jpg" width="600px" /></center>

<p><b> Naive Bayes Model 3</b></p>
<p>This is the third naive bayes model. In this model, the hyperparameter are alpha = 0. In this model, the accuracy is 94%.</p>
<p><center>The snapshot of the accuracy/heatmap is attached below. </center></p>
<center><img alt="nb3_python.jpg" height="400px" src="nb3_python.jpg" width="600px" /></center>

<h3><b>Conclusion</b></h3>
<p>The Naive Bayes model classified tweets generated from Twitter into the hashtag class (socialworker and covid) of different tweets. The accuracy of the models are 95% and 94%. The accuracy is pretty high. With the collection of words, the model is able to predict or classify the tweets into particular classes.  </p>




</div>








</HTML>